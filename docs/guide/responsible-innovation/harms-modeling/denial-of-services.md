---
title: Denial of consequential services
titleSuffix: Azure Application Architecture Guide
description: Automated decisions could limit access to resources, services, and opportunities essential to well-being.
author: dcass
ms.date: 04/22/2020
ms.topic: guide
ms.service: architecture-center
ms.category:
  - fcp
ms.subservice: reference-architecture
---

# Type of harm: Denial of consequential services

_Online retailer Amazon, whose global workforce was 60 percent male and where men hold 74 percent of the company's managerial positions, discontinued use of a recruiting algorithm after discovering gender bias. Image: job seekers lined up at a career fair in NY (Reuters / Shannon Stapleton)_

## Opportunity loss

Automated decisions could limit access to resources, services, and opportunities essential to well-being.

## Employment discrimination

Denying people access to apply for or secure a job based on characteristics unrelated to merit.

- Are there ways in which this technology could impact recommendations or decisions related to employment?

*Example: Hiring AI could recommend fewer candidates with female-sounding names for interviews.*

## Housing discrimination

Denying people access to housing or the ability to apply for housing.

- Could this technology be used to determine access, cost, allocation of insurance, or social benefits?

*Example: Public housing queuing algorithm could cause people with international-sounding names to wait longer for vouchers.*

## Insurance and benefit discrimination

Denying people insurance, social assistance, or access to a medical trial due to biased standards.

- Could this technology be used to determine access, cost, allocation of insurance or social benefits?

*Example: Insurance company might charge higher rates for drivers working night shifts due to algorithmic predictions suggesting increased drunk driving risk.*

## Educational discrimination

Denying people access to education because of an unchangeable characteristic.

- How might this technology be used to determine access, cost, accommodations, or other outcomes related to education?

*Example: Emotion classifier could incorrectly report that students of color are less engaged than their white counterparts, leading to lower grades.*

## Digital divide / technological discrimination

Disproportionate access to the benefits of technology, leaving some people less informed or less equipped to participate in society.

- What prerequisite skills, equipment, or connectivity are necessary to get the most out of this technology?
- What might be the impact of select people gaining earlier access to this technology than others (equipment, connectivity, or other product functionality)?

*Example: Content throttling could prevent rural students from accessing classroom instruction video feed.*

## Loss of choice / network and filter bubble

Presenting people with only information that conforms to and reinforces their own beliefs.

- How might this technology affect which choices and information are made available to people?
- What past behaviors or preferences might this technology rely on to predict future behaviors or preferences?

*Example: News feed could only present information that confirms existing beliefs.*

Economic loss

Automating decisions related to financial instruments, economic opportunity, and resources can amplify existing societal inequities and obstruct well-being.

## Credit discrimination

Denying people access to financial instruments based on characteristics unrelated to economic merit.

- How might this technology rely on existing credit structures to make decisions?
- How might this technology affect the ability of an individual or group to obtain or maintain a credit score?

*Example: Higher introductory rate offers could be sent only to homes in lower socioeconomic postal codes.*

## Differential pricing of goods and services

Offering goods or services at different prices for reasons unrelated to the cost of production or delivery.

- How could this technology be used to determine pricing of goods or services?
- What is the criteria for determining the cost to people for use of this tech?

*Example: More could be charged for products based on designation for men or women.*

## Economic slavery

Compelling or misleading people to work on something that impacts their dignity or wellbeing.

- What role did human labor play in producing training data for this technology? How was this workforce acquired?
- What role does human labor play in supporting this technology? Where is this workforce expected to come from?

*Example: Paying financially destitute people for their biometric data to train AI systems.*

## Devaluation of individual expertise

Technology supplanting the use of paid human expertise or labor.

- How might this technology impact the need to employ an existing workforce?

*Example: AI agents replace doctors/radiographers for evaluation of medical imaging.*

**Reference Docs**

- [Responsible AI resource center](../index.md)
- [Assessing Harms booklet](downloadable)

**Next Steps**

- [Assessing harms](./index.md)
- [Who may be impacted](./human-understanding.md)
- [Type of harm: Infringement on human rights](./human-rights.md)
- [Type of harm: Erosion of social & democratic structures](./democratic-structures.md)