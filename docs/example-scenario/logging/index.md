---
title: title
titleSuffix: Azure Example Scenarios
description: description
author: githubalias
ms.date: 03/01/2020
ms.topic: example-scenario
ms.service: architecture-center
ms.subservice: example-scenarios
ms.custom:
  - fcp
---
In a traditional on-premise application, most applications were written in a non distributed manner. The application was generally made up of a single executing and relevant dependencies. This executable would run under a single process space and would be installed onto a single virtual machine. If there were a need for additional performance, the application would be horizontally scaled across multiple machines. In each instance, logging and tracing and monitoring would be bound to the single process domain. 

With the advent of the cloud and adoption, cloud-native development differs somewhat from the on-premise methodology.

Cloud-native applications, tend to be composed of one or more PaaS services built around the microservices architectural paradigm. This paradigm entails the creation of discrete loosely coupled microservices that work within their process boundary. This loosely distributed architecture brings many benefits – 

•	the application is composed of discrete services that are easier to build and simpler to maintain
•	The Microservices focuses on business capabilities
•	Microservices naturally work well wit automated CI and CD Systems
•	Microservices being naturally independent are more fault-tolerant to failures, i.e. a single service failure will not bring down the application
•	Microservices can scale independently of each other and allow for better utilisation and cost optimisation

Wherever an application will run, there is always a possibility that during the application lifetime that failures, transient or otherwise will occur. When these events happen, teams need to know the following:
•	Why did the application fail? 
o	When did the application exception occur?
o	Which method caused the exception?
o	Were the events recorded up to the point of application failure?

To allow us to answer the above, we must rely on the process of logging, tracing and monitoring.

Logging is the ability to track and report related data in a centralised way. These log events can be used to track errors in code, application failures, or purely informational messages. Logging focuses on providing an overview of the state of an application execution through the use of discrete messages, or, events. Through these logs, we can put automation in place in which events can be read, and relevant parties can be notified if a criterion or threshold has been met.

Tracing, on the other hand, focuses on the continuous flow of the application, tracing allows us to trace the program execution from beginning to end through the various methods and services while understanding the state and transitions of data.

Monitoring can, at times, be used to mean tracing or logging. However, monitoring is the process in which application instrumentation is used to provide metrics in which a user can reason about and thus make informed decisions. These metrics can be aggregated data from logs or trace events into a dashboard view that allows operations teams a holistic view of the application health from utilisation to error count.

For single-process applications that traditionally run on-premises, logging and tracing is a relatively straightforward process. 

The application and it's dependencies are deployed together, logging and tracing within a single execution context. All relevant calls within the application happen within the same process boundary, and there is no need to cross-application or process boundaries.

When developing cloud-native distributed applications, this can be a somewhat complicated endeavour and at times. When troubleshooting a problem Monoliths are convenient as all code is bound withing a single execution context and so it is easier to track a request and trace the code path through the entirety of the request cycle. 

A cloud-native application, being distributed by design, a single request can interact with many microservices. Each microservice will generate its own set of logs from their specific task, and it becomes problematic to determine the process flow of execution.

Teams tend to process the log independently for each microservice, as services can handle hundreds of requests concurrently, this can become a laborious task of wading through logs and manually determining a correlation of events.

To overcome logging challenges, the following practices can be adopted in a distributed architecture.

•	All requests generated by the user application should have a unique identifier for the request. This unique identifier shall be passed through each microservice. Each microservice should be designed to accept this Correlation ID as part of the request and all logs emitted by the service should contain the correlation ID.
•	When the request is processed, and the response sent, the correlation ID should be returned as part of the response. The user application should then use the correlation ID when emitting its own logs.
•	All logs should be emitted to a single hub and stored in a central repository
•	Log data should be structured using JSON 

By incorporating the above changes to the distributed application, it now allows for any member of a team to retrieve logs from the complete lifecycle of the request through the correlation ID. 

All logs should be stored in long term storage; this allows for analysis and diagnosis of issues and also allows the team to determine if there have been changes to system behaviour over time.

Context should be added to every request, and these objects can be:
•	Date and Time such as UTC
•	Service name
•	HTTP Codes
•	Browser Type
•	Severity of Event
•	Pertinent information from the request type that can be used to help diagnose problems, i.e. method name.

Care must be taken to ensure the recorded information does not contain PII data and meets any regulatory guidelines such as GDPR.

Azure provides rich services to implement an effective Logging/Tracing and Monitoring Strategy.
•	Event Hub
•	Data Lake
•	Stream Analytics
•	Logic Apps
•	Application Insights
•	Azure Monitor

Azure Event Hub is a fully managed, real-time ingestion service that is able to stream large volumes of events per second. As such, Event Hubs is the perfect candidate for a central log ingestion pipeline <Link thttps://azure.microsoft.com/services/event-hubs/ further information> 

Event hub can be configured to send all event message to Azure Data Lake for long term archival for analysis.

Azure Data Lake is a cloud scalable storage repository that can be used to store data in any format and for long periods of time. Developers can then query the objects stored within the Data Lake for investigational purposes <Link https://azure.microsoft.com/services/storage/data-lake-storage/ for further information>.

Azure Stream Analytics is a real-time serverless analytics engine design for critical machine workloads. Stream analytics can be leveraged to process event messages if critical indicators are met <Link https://azure.microsoft.com/services/stream-analytics/for further information>. 

Azure Logic Apps is a serverless cloud service that allows developers to schedule, orchestrate common tasks through a series of workflows via a visual designer. The power of logic apps is through the connectors that be used to integrate with a number of first and third party services <Link https://docs.microsoft.com/azure/logic-apps/logic-apps-overview for further information>.

These Stream Analytics Jobs can then be used to trigger a Logic App Workflow. The workflow can incorporate notification elements, to notify the development team as well as the ability to call RESTful APIs. 

Application Insights is an extensible Application Performance Management service for Developers and the DevOps Team. Application Insights can be used to monitor live services, detect anomalies in performance and analytics tools to diagnose and trace problems and query log data as well as diagnose issues using telemetry from Application Insights within Visual Studio <Link https://docs.microsoft.com/azure/logic-apps/logic-apps-overview for further information>

Application Insights can be leveraged to for distributed tracing through the use of the SDK <Link https://docs.microsoft.com/azure/azure-monitor/app/distributed-tracing for further information>

Azure Monitor is a service that maximises the availability and performance of cloud-native applications. Collecting, analysing and acting on telemetry from cloud-native applications. With Azure, Monitor Teams can create operational Dashboard and detect issues and the ability to Alert Teams of critical situations <Link https://docs.microsoft.com/azure/azure-monitor/overview for further information>/

If the team leverages an ITSM system, Logic Apps can be used to call the RESTful endpoint of the ITSM system and create the relevant issue with the appropriate severity level <Link https://docs.microsoft.com/archive/blogs/vinaysin/consuming-azure-stream-analytics-output-in-azure-logic-apps for further information (Need to test as this looks deprecated)>. This allows for quicker notification to all relevant teams and ensures that triaging is more immediate and useful.

When building a cloud-native distributed microservices architure, teams are able to leverage these Azure Services and build an effecting Logging; Tracing and Monitoring Solution