---
title: Health modeling for mission-critical workloads on Azure
description: Reference architecture for a workload that is accessed over a public endpoint without additional dependencies to other company resources - Health modeling.
author: nielsams
categories: monitoring
ms.author: allensu
ms.date: 08/15/2022
ms.topic: conceptual
ms.service: architecture-center
ms.subservice: reference-architecture
ms.category:
  - monitoring
azureCategories:
  - monitoring  
summary: Reference architecture for a workload that is accessed over a public endpoint without additional dependencies to other company resources.
products:
  - azure-monitor
---

# Health modeling for mission-critical workloads

Monitoring applications and infrastructure is an important part of any infrastructure deployment. For a mission-critical workload, monitoring is a critical part of the deployment. Monitoring application health and key metrics of Azure resources helps you understand if the environment is working as expected.

To fully understand these metrics and evaluate the overall health of a workload requires a holistic understanding of all of the data monitored. A health model can assist with evaluation of the overall health status by displaying a clear indication of the health of the workload instead of raw metrics. The status is often presented as "traffic light" indicators such as red, green, or yellow. Representation of a health model with clear indicators makes it intuitive for an operator to understand the overall health of the workload and respond quickly to issues that arise.

Health modeling can be expanded into the following operational tasks for the mission-critical deployment:

- **Application Health Service** - Application component on the compute cluster that provides an API to determine the health of a stamp.

- **Monitoring** - Collection of performance and application counters that evaluate the health and performance of the application and infrastructure.

- **Alerting** - Actionable alerts of issues or outages in the infrastructure and application.

- **Failure analysis** - Breakdown and analysis of any failures including documentation of root cause.

Taken together, these tasks make up a comprehensive health model for the mission-critical infrastructure. Development of a health model can and should be an exhaustive and integral part of any mission-critical deployment.

## Application Health Service

The Application Health Service (HealthService) is an application component that resides with the Catalog Service (CatalogService) and the Background Processor (BackgroundProcessor) within the compute cluster. The **HealthService** provides a REST API for Azure Front Door to call to determine the health of a stamp. The **HealthService** is a complex component that reflects the state of dependencies, in addition to it's own.

When the compute cluster is down, the health service won't respond. When the services is up and running, it performs periodic checks against the following components in the infrastructure:

- It attempts to do a simple query against Cosmos DB.

- It attempts to send a message to Event Hub. The message is filtered out by the background worker.

- It looks up a state file in the storage account. This file can be used to turn off a region, even while the other checks are still operating correctly.

All health check results are cached in memory for a configurable number of seconds, by default 10. The cached results in not every call to the API results in backend calls. This operation does add a small potential latency in detecting outages. Caching the results reduces additional cluster load generated by health checks.

### Configuration

The **HealthService** and the **CatalogService** have configuration settings common between the components except for the following settings used exclusively by the **HealthService**:

| Setting | Value |
| ------- | ----- |
| **`HealthServiceCacheDurationSeconds`** | Controls the expiration time of memory cache, in seconds. |
| **`HealthServiceStorageConnectionString`** | Connection string for the Storage Account where the status file should be present. |
| **`HealthServiceBlobContainerName`** | Storage Container where the status file should be present. |
| **`HealthServiceBlobName`** | Name of the status file - health check will look for this. |
| **`HealthServiceOverallTimeoutSeconds`** | Timeout for the whole check - defaults to 3 seconds. If the check doesn't finish in this interval, the service reports unhealthy. |

### Implementation

All checks are performed asynchronously and **in parallel**. If either of them fails, **the whole stamp will be considered unavailable.**

Check results are cached in memory, using the standard, non-distributed ASP.NET Core **`MemoryCache`**. Cache expiration is controlled by **`SysConfig.HealthServiceCacheDurationSeconds`** and is set to 10 seconds by default.

> [!NOTE]
> The **`SysConfig.HealthServiceCacheDurationSeconds`** configuration setting reduces the additional load generated by health checks as not every request will result in downstream call to the dependent services.

The following table details the health checks for the components in the infrastructure:

| Component | Health check |
| --------- | ------------ |
| **Storage account Blob** | The blob check currently serves two purposes: </br> 1. Test if it's possible to reach Blob Storage. The storage account is used by other components in the stamp and is considered a critical resource. </br> 2. Manually "turn off" a region by manipulating (i.e. deleting) the state file. </br> A design decision was made that the check would **only look for a presence of a state file** in the specified blob container. The content of the file isn't processed. There is a possibility to setup a more sophisticated system that reads the content of the file and return different status based on the content of the file. </br> Examples of content are **HEALTHY**, **UNHEALTHY**, and **MAINTENANCE**. </br> Removal of the state file will disable the stamp. Ensure the health file is present after deploying the application. Absence of the health file will cause the service to always respond with **UNHEALTHY**. Front Door won't recognize the backend as available. </br> The file is created by Terraform and should be present after the infrastructure deployment. |
| **Event Hub** | Event Hub health reporting is handled by the **`EventHubProducerService`**. This service reports healthy if it's able to send a new message to Event Hub. For filtering, this message has an identifying property added to it: </br> **`HEALTHCHECK=TRUE`** </br> This message is ignored on the receiving end. The **`AlwaysOn.BackgroundProcessor.EventHubProcessorService.ProcessEventHandlerAsync()`** configuration setting checks for the **`HEALTHCHECK`** property. |
| **Cosmos DB** | Cosmos DB health reporting is handled by the **`CosmosDbService`**, which reports healthy if it is: </br> 1. Able to connect to Cosmos DB database and perform a simple query. </br> 2. Able to write a test document to the database. </br> The test document has a very short Time-to-Live set, Cosmos DB automatically removes it. </br> The **HealthService** performs two separate probes. The two probes ensure that if Cosmos DB is in a state where reads still work, but writing documents doesn't', an alert can be triggered. |

#### Cosmos DB queries

For the Read-only query, the following query is being used, which doesn't fetch any data and doesn't have a large impact on overall load:

```sql
SELECT GetCurrentDateTime ()
```

The write query creates a dummy `ItemRating` with minimum content:

```csharp
var testRating = new ItemRating()
{
    Id = Guid.NewGuid(),
    CatalogItemId = Guid.NewGuid(), // Create some random (=non-existing) item id
    CreationDate = DateTime.UtcNow,
    Rating = 1,
    TimeToLive = 10 // will be auto-deleted after 10sec
};

await AddNewRatingAsync(testRating);
```

## Monitoring

Azure Log Analytics is used as the central store fo logs and metrics for all application and infrastructure components. Azure Application Insights is used for all application monitoring data. Each stamp in the infrastructure has a dedicated Log Analytics workspace and Application Insights instance. A separate Log Analytics workspace is used for the globally shared resources such as Front Door and Cosmos DB.

All stamps are short-lived and continuously replaced with each new release. The per-stamp Log Analytics workspaces are deployed as a global resource in a separate monitoring resource group as the stamp Log Analytics resources. These resources don't share the lifecycle of a stamp.

## Monitoring: Data sources

- **Diagnostic settings**: All Azure services used for Azure Mission-Critical are configured to send all their Diagnostic data including logs and metrics to the deployment specific (global or stamp) Log Analytics Workspace. This happens automatically as part of the Terraform deployment. New options will be identified automatically and added as part of `terraform apply`.

- **Kubernetes monitoring**: Diagnostic settings are used to send AKS logs and metrics to Log Analytics. AKS is configured to use **Container Insights**. Container Insights deploys the **OMSAgentForLinus** via a Kubernetes DaemonSet on each node in the AKS clusters. The OMSAgentForLinux is capable of collecting additional logs and metrics from within the Kubernetes cluster and sends them to its corresponding Log Analytics workspace. This contains more granular data about pods, deployments, services and the overall cluster health. To gain more insights form the various components like ingress-nginx, cert-manager, and other components deployed to Kubernetes next to the mission-critical workload, it's possible to use [Prometheus scraping](/azure/azure-monitor/containers/container-insights-prometheus-integration). Prometheus scraping configures the OMSAgentForLinux to scrape Prometheus metrics from various endpoints within the cluster. |
 
## Monitoring: Application Insights availability tests

To monitor the availability of the individual stamps and the overall solution from an outside point of view, [Application Insights Availability Tests](/azure/azure-monitor/app/availability-overview) are set up in two places:

- **Regional availability tests**: These tests are setup in the regional Application Insights instances and are used to monitor the availability of the stamps. These tests target the clusters as well as the static storage accounts of the stamps directly. To call the ingress points of the clusters directly, requests need to carry the correct Front Door ID header, otherwise they are rejected by the ingress controller.

- **Global availability test**: These tests are setup in the global Application Insights instance and are used to monitor the availability of the overall solution by pinging Front Door. Two tests are used: One to test an API call against the **CatalogService** and one to test the home page of the website.

## Monitoring: Queries

Azure Mission-Critical uses different Kusto Query Language (KQL) queries to implement complex, custom queries as functions to retrieve data from Log Analytics. These queries are stored as individual files in the separated into global and stamp and are imported and applied automatically via Terraform as part of each infrastructure pipeline run.

This approach separates the query logic from the visualization layer. It allows calls to these functions individually and use them either directly to retrieve data from Log Analytics or to visualize the results in Azure Dashboards, Azure Monitor Workbooks or 3rd-Party dashboard solutions like Grafana.

## Monitoring: Visualization

The visualization of the Kusto Queries described previously are implemented with Grafana. Grafana is used to show the results of Log Analytics queries and doesn't contain any logic itself. The Grafana stack isn't part of the solution's deployment lifecycle, but released separately.

## Alerting

Alerts are an important part of the overall operations strategy. Proactive monitoring such as the use of dashboards should be used in conjunction with alerts that raise immediate attention to issues.

While most critical alert rules should be defined during the building of an application, rules will always require refinement over time. Outages caused by errors that went undetected, often lead to the creation of additional monitoring point and alert rules. Alerts could be delivered as emails, mobile push notifications, or tickets created in an IT Service Management system. The important part is that they get routed to a place where they will be noticed and acted upon quickly.

A full definition and implementation of alert rules would go beyond the scope of the reference implementation. Only a couple of examples are implemented. In the reference implementation it uses email notifications. oOher alert sinks can be configured using Terraform. To avoid unnecessary noise, alerts are not created in the E2E environments.

Alerts in Azure can be configured at various levels. For each category we define a couple of sample alerts which we consider most valuable. Not all of the samples are actually implemented in the reference implementation, but other alerts can follow the same route.

### Resource level alerts

Resource level alerts are configured on an Azure resource itself. The alerts are scoped to only that resource and does not correlate with signals from other resources.

The following table lists example metric alerts for different components of the reference architecture.Metric alerts are limited to the built-in metrics that Azure provides for a given resource.

| Service | Alert |
| ------- | ----- |
| **Front Door** | Health of backend drops under a set threshold. |
| **Cosmos DB** | Availability drops under a set threshold. |
| **Cosmos DB** | RU consumption percentage reaching a set threshold. |
| **Event Hub Namespace** | Quota exceeded errors or throttled requests greater than 0. |
| **Event Hub Namespace** | Outgoing messages drop to 0. |
| **Key Vault** | Overall vault availability drops under a set threshold. |
| **AKS** | Unschedulable pods greater than 0 for sustained period. |
| **Storage account** | Availability drops under a set threshold. |

#### Front Door - Backend health

A backend health alert is implemented as a sample as part of the reference implementation. A metric alert is configured a part of the infrastructure deployment on Front Door. The alert uses the **Backend health percentage** metric to create an alert when of the backend's health falls under a set threshold within a minute.

Many causes for the backend health to drop should be detected on other levels or potentially earlier. Anything that causes the Health Service to report unhealthy to Front Door's health probes should be logged to Application Insights. Issues with the static storage accounts should be detected through collected diagnostic logs. Even with increased visibility with logging and alerting, there can still be outages which aren't showing up in other signals.

Front Door doesn't provide any further insight in to why the backend health for a given backend drops. The reference infrastructure implements URL ping tests in each stamp's Application Insights resource. The URL test calls the same URL of the cluster **HealthService** and checks the static website storage account. Front Door provides detailed logging and tracing. The logging and tracing can be used to help determine the cause of an outage.

### Log Analytics / Application Insights query-based alerts

Alerts based on the data stored in a Log Analytics can be created with any arbitrary query. Queries in Log Analytics are well suited for correlation of events from multiple sources. Log Analytics queries can be used to create alerts based on application level signals as opposed to only resource level events and metrics.

#### Valuable alerts

- Percentage of 5xx responses / failed requests exceeding a threshold.

- The result of the **`AksClusterHealthScore()`** function dropping below 1.

- Spike in entries in the exception table. Not all errors are correlated to incoming requests so they won't be covered by the previous alert, for instance exceptions in the **BackgroundProcessor**.

#### Percentage of 5xx responses / failed requests exceeding a threshold

The 5xx responses alert is created as part of the reference implementation. To demonstrate their setup and usage, a query-based alert on Application Insights is configured as part of the infrastructure deployment within each stamp. The alert looks at the number of responses sent by the **CatalogService** which start with a 5xx status code. If the 5xx status code exceed the set threshold within a five minute window, it will fire an alert.

## Failure analysis

Composing the failure analysis is mostly a theoretical planning exercise. It can, and should be complemented by actual failure injection testing. Through testing, at least some of the failure cases and their impact can be simulated and thus validate the theoretical analysis.

The following table list possible failure scenarios of the various components of the Azure Mission-Critical reference implementation. The table lists risks for the individual components and evaluates if their failure can cause an outage of the entire application. This list isn't a complete list, as there can be failure cases which haven't been thought of.

| Service | Risk | Impact/Mitigation/Comment | Outage |
| ------- | ---- |-------------------------- | ------ |
| **Azure Active Directory** | Azure AD becomes unavailable. | Currently no possible mitigation in place. Multi-region approach will likely not (fully) mitigate any outages here as it's a global service. This is a hard dependency. AAD is being used for control plane operations like the creation of new AKS nodes, pulling container images from ACR or to access Key Vault on pod startup. It's expected that existing, running components should be able to keep running when AAD experiences issues. It's likely that new pods or AKS nodes will be unable to spawn. In scale operations are required during this time, it could lead to a decreased user experience and potentially to outages. | Partial |
| **Azure DNS** | Azure DNS becomes unavailable and DNS resolution fails. | If Azure DNS becomes unavailable, the DNS resolution for user requests and between different components of the application will likely fail. Currently no possible mitigation in place for this scenario. Multi-region approach will likely not (fully) mitigate any outages here as it's a global service. Azure DNS is a hard dependency. External DNS services as backup would not help, since all the PaaS components used rely on Azure DNS. Bypassing DNS by switching to IP isn't an option. Azure services don’t have static, guaranteed IP addresses. | Full |
| **Front Door** | General Front Door outage. | If Front Door goes down entirely, there isn't a mitigation. This is a hard dependency. | Yes |
| **Front Door** | Routing/frontend/backend configuration errors. | Can happen due to mismatch in configuration when deploying. Should be caught in testing stages. Some things like frontend configuration with DNS is specific to each environment. **Mitigation: Rolling back to previous configuration should fix most issues.**. As changes take a couple of minutes in Front Door to deploy, it will cause an outage. | Full |
| **Front Door** | Managed SSL certificate is deleted. | Can happen due to mismatch in configuration when deploying. Should be caught in testing stages. Technically the site would still work, but SSL cert errors will prevent users from accessing it. **Mitigation: Re-issuing the cert can take around 20 minutes, plus fixing and re-running the pipeline.**. | Full |
| **Cosmos DB** | 



## Next steps

Deploy the reference implementation to get a full understanding of the resources and their configuration used in this architecture.

> [!div class="nextstepaction"]
> [Implementation: Mission-Critical Online](https://github.com/Azure/Mission-Critical-Online)
